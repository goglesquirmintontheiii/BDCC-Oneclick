{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE1BHRAe1xPq"
      },
      "source": [
        "# Install the New Worker- No more BETA!!\n",
        "### Remember to first set your API KEY and worker name\n",
        "#### You can serve different models, simply change the name in models_to_load to match the model you want, you can check either https://aqualxx.github.io/stable-ui/workers, in the models tab, or https://tinybots.net/artbot/info/models, or https://aihorde.sitew3.com/ also in the models tab (this one also includes text gen models); just copy paste the name (it's set to Deliberate by default)\n",
        "#### You can also change max_power and see how high you can go, it's set to 20 by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsFfUpesrry6"
      },
      "outputs": [],
      "source": [
        "# @title ## Only to obtain the models names from the horde. If you want it, tick the checkbox here and run this cell. It does nothing for the worker, it will simply print a list of all the models (Outdated, pending){ display-mode: \"form\" }\n",
        "get_models = False # @param {type:\"boolean\"}\n",
        "\n",
        "if (get_models):\n",
        "\n",
        "  !wget -q -O /content/stable_diffusion.json https://raw.githubusercontent.com/Haidra-Org/AI-Horde-image-model-reference/main/stable_diffusion.json\n",
        "\n",
        "  with open(\"/content/stable_diffusion.json\", 'r') as file:\n",
        "      lines = file.readlines()\n",
        "\n",
        "  delete_name = '        \"name\": '\n",
        "  delete_enter = '\\n'\n",
        "  model_list = []\n",
        "\n",
        "  for i, line in enumerate(lines):\n",
        "      if delete_name in line:\n",
        "          #print(f\"{line.replace(delete_name, '').replace(delete_enter,'')}\")\n",
        "          #model_list.append = [f\"{line.replace(delete_name, '').replace(delete_enter,'')}\"]\n",
        "          model_list.append(line.replace(delete_name, '').replace(delete_enter,''))\n",
        "\n",
        "  model_list.sort()  # Sort the model_list in alphabetical order\n",
        "\n",
        "  #print(model_list)\n",
        "  print(' '.join(model_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PzEAmH3rry7"
      },
      "outputs": [],
      "source": [
        "# @title # Simple Timer AND Clearscreen functions { display-mode: \"form\" }\n",
        "from IPython.display import Javascript\n",
        "\n",
        "# JavaScript function to clearscreen\n",
        "js_code = \"\"\"\n",
        "function clearScreen() {\n",
        "  document.body.innerHTML = \"\";\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# JavaScript functions to show timer\n",
        "js_code_2 = \"\"\"let startTime = new Date().getTime(); // Variable to store the start time\n",
        "\n",
        "function formatTime(seconds) {\n",
        "  const hours = Math.floor(seconds / 3600);\n",
        "  const minutes = Math.floor((seconds % 3600) / 60);\n",
        "  const remainingSeconds = seconds % 60;\n",
        "  return `${hours} hours, ${minutes} minutes, ${remainingSeconds} seconds`;\n",
        "}\n",
        "\n",
        "function updateTime() {\n",
        "  const currentTime = new Date().getTime();\n",
        "  const elapsedSeconds = Math.floor((currentTime - startTime) / 1000);\n",
        "  const formattedTime = formatTime(elapsedSeconds);\n",
        "  document.body.innerHTML = formattedTime;\n",
        "}\"\"\"\n",
        "\n",
        "# Set the interval for the function to run\n",
        "interval = \"1000\"\n",
        "\n",
        "# Display the JavaScript code in the cell\n",
        "display(Javascript(js_code_2 + f\"setInterval(updateTime, {interval})\"))\n",
        "\n",
        "def python_clearscreen_now():\n",
        "    display(Javascript(js_code + \"clearScreen()\"))\n",
        "\n",
        "def python_clearscreen_interval(interval):\n",
        "    display(Javascript(js_code + f\"setInterval(clearScreen, {interval})\"))\n",
        "\n",
        "\n",
        "\n",
        "# Function to clear outputs using Python's threads\n",
        "\n",
        "import threading\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def clear_output_periodically(interval):\n",
        "  \"\"\"\n",
        "  Clears the output cell periodically until the stop_thread flag is set.\n",
        "\n",
        "  Args:\n",
        "    interval: The interval in seconds between clearing the output.\n",
        "  \"\"\"\n",
        "  global stop_thread\n",
        "  while not stop_thread:\n",
        "    clear_output(wait=True)\n",
        "    time.sleep(interval)\n",
        "\n",
        "stop_thread = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-20T23:06:35.113455Z",
          "iopub.status.busy": "2023-10-20T23:06:35.112701Z"
        },
        "id": "qHYIcsqJ1xPs",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 0.- This cell will set the variables and rerun the worker if you stopped it, but only if everything else was installed already { display-mode: \"form\" }\n",
        "\n",
        "import os\n",
        "\n",
        "#### For now, THESE are the only variables that we care about\n",
        "worker_name = \"Collabworker\" #@param {type:\"string\"}\n",
        "api_key = \"\" #@param {type:\"string\"}\n",
        "civitai_token = \"cae554bcc138d97a9323856c2dee1158\" #@param {type:\"string\"}\n",
        "max_power = 20  #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "selected_models = []\n",
        "#@markdown  ### Select what model(s) to host\n",
        "#@markdown  Recommended models, (bypasses the \"all models\" selector below when set)\n",
        "recommended_model1 = \"ICBINP - I Can't Believe It's Not Photography\" # @param [\"None\", \"ICBINP - I Can't Believe It's Not Photography\", \"Anything Diffusion\", \"Deliberate\", \"Dreamshaper\", \"BB95 Furry Mix\", \"Hentai Diffusion\", \"Rev Animated\", \"AlbedoBase XL (SDXL)\", \"Fustercluck\", \"ICBINP XL\", \"Animagine XL\", \"Anime Illust Diffusion XL\", \"Juggernaut XL\", \"Pony Diffusion XL\", \"DreamShaper XL\", \"Cheyenne\", \"Unstable Diffusers XL\", \"Stable Cascade 1.0\"]\n",
        "recommended_model2 = \"None\" # @param [\"None\", \"ICBINP - I Can't Believe It's Not Photography\", \"Anything Diffusion\", \"Deliberate\", \"Dreamshaper\", \"BB95 Furry Mix\", \"Hentai Diffusion\", \"Rev Animated\", \"AlbedoBase XL (SDXL)\", \"Fustercluck\", \"ICBINP XL\", \"Animagine XL\", \"Anime Illust Diffusion XL\", \"Juggernaut XL\", \"Pony Diffusion XL\", \"DreamShaper XL\", \"Cheyenne\", \"Unstable Diffusers XL\", \"Stable Cascade 1.0\"]\n",
        "\n",
        "#@markdown List of all models\n",
        "model1 = \"Anything Diffusion\" # @param [\"3DKX\", \"526Mix-Animated\", \"A-Zovya RPG Inpainting\", \"ACertainThing\", \"AIO Pixel Art\", \"Abyss OrangeMix\", \"AbyssOrangeMix-AfterDark\", \"AlbedoBase XL (SDXL)\", \"Analog Diffusion\", \"Analog Madness\", \"Animagine XL\", \"Anime Illust Diffusion XL\", \"Anime Pencil Diffusion\", \"AnyLoRA\", \"Anygen\", \"Anything Diffusion Inpainting\", \"Anything Diffusion\", \"Anything v3\", \"Anything v5\", \"App Icon Diffusion\", \"Art Of Mtg\", \"Aurora\", \"BB95 Furry Mix\", \"BPModel\", \"BRA\", \"Babes\", \"BweshMix\", \"CamelliaMix 2.5D\", \"Cetus-Mix\", \"Char\", \"CharHelper\", \"Cheese Daddys Landscape Mix\", \"Cheyenne\", \"ChilloutMix\", \"ChromaV5\", \"Classic Animation Diffusion\", \"Colorful\", \"Comic-Diffusion\", \"Counterfeit\", \"CyberRealistic\", \"CyriousMix\", \"DGSpitzer Art Diffusion\", \"Dan Mumford Style\", \"Dark Sushi Mix\", \"Dark Victorian Diffusion\", \"Deliberate 3.0\", \"Deliberate Inpainting\", \"Deliberate\", \"Disco Elysium\", \"Disney Pixar Cartoon Type A\", \"DnD Item\", \"DnD Map Generator\", \"Double Exposure Diffusion\", \"DreamLikeSamKuvshinov\", \"DreamShaper Inpainting\", \"DreamShaper XL\", \"Dreamlike Diffusion\", \"Dreamlike Photoreal\", \"Dreamshaper\", \"DucHaiten Classic Anime\", \"DucHaiten\", \"Dungeons and Diffusion\", \"Dungeons n Waifus\", \"Edge Of Realism\", \"Eimis Anime Diffusion\", \"Elldreth's Lucid Mix\", \"Elysium Anime\", \"Epic Diffusion Inpainting\", \"Epic Diffusion\", \"Ether Real Mix\", \"ExpMix Line\", \"Experience\", \"FaeTastic\", \"Fantasy Card Diffusion\", \"Fluffusion\", \"Funko Diffusion\", \"Furry Epoch\", \"Fustercluck\", \"GTA5 Artwork Diffusion\", \"Galena Redux\", \"Ghibli Diffusion\", \"GhostMix\", \"GorynichMix\", \"Grapefruit Hentai\", \"Graphic-Art\", \"GuFeng\", \"GuoFeng\", \"HASDX\", \"HRL\", \"Hassaku\", \"Hassanblend\", \"Healy's Anime Blend\", \"Henmix Real\", \"Hentai Diffusion\", \"ICBINP - I Can't Believe It's Not Photography\", \"ICBINP XL\", \"Illuminati Diffusion\", \"Inkpunk Diffusion\", \"JWST Deep Space Diffusion\", \"Jim Eidomode\", \"JoMad Diffusion\", \"Juggernaut XL\", \"Kenshi\", \"Laolei New Berry Protogen Mix\", \"Lawlas's yiff mix\", \"Liberty\", \"Lyriel\", \"Mega Merge Diffusion\", \"MeinaMix\", \"Microcritters\", \"Microworlds\", \"Midjourney PaintArt\", \"Mistoon Amethyst\", \"ModernArt Diffusion\", \"Moedel\", \"MoistMix\", \"MoonMix Fantasy\", \"Movie Diffusion\", \"Neurogen\", \"NeverEnding Dream\", \"Nitro Diffusion\", \"OpenJourney Diffusion\", \"Openniji\", \"PFG\", \"PPP\", \"Papercut Diffusion\", \"Pastel Mix\", \"Perfect World\", \"Poison\", \"Pokemon3D\", \"Pony Diffusion XL\", \"PortraitPlus\", \"Pretty 2.5D\", \"Project Unreal Engine 5\", \"ProtoGen\", \"Protogen Anime\", \"Protogen Infinity\", \"Pulp Vector Art\", \"RPG\", \"Ranma Diffusion\", \"Real Dos Mix\", \"RealBiter\", \"Realisian\", \"Realism Engine\", \"Realistic Vision Inpainting\", \"Realistic Vision\", \"Reliberate\", \"Rev Animated\", \"Robo-Diffusion\", \"SD-Silicon\", \"SDXL 1.0\", \"Samaritan 3d Cartoon\", \"Sci-Fi Diffusion\", \"Seek.art MEGA\", \"Something\", \"Stable Cascade 1.0\", \"SweetBoys 2D\", \"ToonYou\", \"Trinart Characters\", \"Tron Legacy Diffusion\", \"UMI Olympus\", \"URPM\", \"Uhmami\", \"Ultraskin\", \"Unstable Diffusers XL\", \"Unstable Ink Dream\", \"Vector Art\", \"VinteProtogenMix\", \"Western Animation Diffusion\", \"Woop-Woop Photo\", \"Yiffy\", \"Zack3D\", \"Zeipher Female Model\", \"iCoMix Inpainting\", \"iCoMix\", \"majicMIX realistic\", \"stable_diffusion\", \"stable_diffusion_2.1\", \"stable_diffusion_inpainting\", \"vectorartz\", \"waifu_diffusion\"]\n",
        "model2 = \"None\" # @param [\"None\", \"3DKX\", \"526Mix-Animated\", \"A-Zovya RPG Inpainting\", \"ACertainThing\", \"AIO Pixel Art\", \"Abyss OrangeMix\", \"AbyssOrangeMix-AfterDark\", \"AlbedoBase XL (SDXL)\", \"Analog Diffusion\", \"Analog Madness\", \"Animagine XL\", \"Anime Illust Diffusion XL\", \"Anime Pencil Diffusion\", \"AnyLoRA\", \"Anygen\", \"Anything Diffusion Inpainting\", \"Anything Diffusion\", \"Anything v3\", \"Anything v5\", \"App Icon Diffusion\", \"Art Of Mtg\", \"Aurora\", \"BB95 Furry Mix\", \"BPModel\", \"BRA\", \"Babes\", \"BweshMix\", \"CamelliaMix 2.5D\", \"Cetus-Mix\", \"Char\", \"CharHelper\", \"Cheese Daddys Landscape Mix\", \"Cheyenne\", \"ChilloutMix\", \"ChromaV5\", \"Classic Animation Diffusion\", \"Colorful\", \"Comic-Diffusion\", \"Counterfeit\", \"CyberRealistic\", \"CyriousMix\", \"DGSpitzer Art Diffusion\", \"Dan Mumford Style\", \"Dark Sushi Mix\", \"Dark Victorian Diffusion\", \"Deliberate 3.0\", \"Deliberate Inpainting\", \"Deliberate\", \"Disco Elysium\", \"Disney Pixar Cartoon Type A\", \"DnD Item\", \"DnD Map Generator\", \"Double Exposure Diffusion\", \"DreamLikeSamKuvshinov\", \"DreamShaper Inpainting\", \"DreamShaper XL\", \"Dreamlike Diffusion\", \"Dreamlike Photoreal\", \"Dreamshaper\", \"DucHaiten Classic Anime\", \"DucHaiten\", \"Dungeons and Diffusion\", \"Dungeons n Waifus\", \"Edge Of Realism\", \"Eimis Anime Diffusion\", \"Elldreth's Lucid Mix\", \"Elysium Anime\", \"Epic Diffusion Inpainting\", \"Epic Diffusion\", \"Ether Real Mix\", \"ExpMix Line\", \"Experience\", \"FaeTastic\", \"Fantasy Card Diffusion\", \"Fluffusion\", \"Funko Diffusion\", \"Furry Epoch\", \"Fustercluck\", \"GTA5 Artwork Diffusion\", \"Galena Redux\", \"Ghibli Diffusion\", \"GhostMix\", \"GorynichMix\", \"Grapefruit Hentai\", \"Graphic-Art\", \"GuFeng\", \"GuoFeng\", \"HASDX\", \"HRL\", \"Hassaku\", \"Hassanblend\", \"Healy's Anime Blend\", \"Henmix Real\", \"Hentai Diffusion\", \"ICBINP - I Can't Believe It's Not Photography\", \"ICBINP XL\", \"Illuminati Diffusion\", \"Inkpunk Diffusion\", \"JWST Deep Space Diffusion\", \"Jim Eidomode\", \"JoMad Diffusion\", \"Juggernaut XL\", \"Kenshi\", \"Laolei New Berry Protogen Mix\", \"Lawlas's yiff mix\", \"Liberty\", \"Lyriel\", \"Mega Merge Diffusion\", \"MeinaMix\", \"Microcritters\", \"Microworlds\", \"Midjourney PaintArt\", \"Mistoon Amethyst\", \"ModernArt Diffusion\", \"Moedel\", \"MoistMix\", \"MoonMix Fantasy\", \"Movie Diffusion\", \"Neurogen\", \"NeverEnding Dream\", \"Nitro Diffusion\", \"OpenJourney Diffusion\", \"Openniji\", \"PFG\", \"PPP\", \"Papercut Diffusion\", \"Pastel Mix\", \"Perfect World\", \"Poison\", \"Pokemon3D\", \"Pony Diffusion XL\", \"PortraitPlus\", \"Pretty 2.5D\", \"Project Unreal Engine 5\", \"ProtoGen\", \"Protogen Anime\", \"Protogen Infinity\", \"Pulp Vector Art\", \"RPG\", \"Ranma Diffusion\", \"Real Dos Mix\", \"RealBiter\", \"Realisian\", \"Realism Engine\", \"Realistic Vision Inpainting\", \"Realistic Vision\", \"Reliberate\", \"Rev Animated\", \"Robo-Diffusion\", \"SD-Silicon\", \"SDXL 1.0\", \"Samaritan 3d Cartoon\", \"Sci-Fi Diffusion\", \"Seek.art MEGA\", \"Something\", \"Stable Cascade 1.0\", \"SweetBoys 2D\", \"ToonYou\", \"Trinart Characters\", \"Tron Legacy Diffusion\", \"UMI Olympus\", \"URPM\", \"Uhmami\", \"Ultraskin\", \"Unstable Diffusers XL\", \"Unstable Ink Dream\", \"Vector Art\", \"VinteProtogenMix\", \"Western Animation Diffusion\", \"Woop-Woop Photo\", \"Yiffy\", \"Zack3D\", \"Zeipher Female Model\", \"iCoMix Inpainting\", \"iCoMix\", \"majicMIX realistic\", \"stable_diffusion\", \"stable_diffusion_2.1\", \"stable_diffusion_inpainting\", \"vectorartz\", \"waifu_diffusion\"]\n",
        "#@markdown Because Colab has low RAM, it is not recommended to run more than 1 model or any of the SDXL models, but you can certainly try (if you do, you might want to set queue size to 0, it will certainly let you run any number of SD 1.5 models, provided you have the disk space for them and you don't mind the long download time and the loading times between models when the worker is actually running)\n",
        "\n",
        "if (recommended_model1 != \"None\"):\n",
        "    selected_models.append(recommended_model1)\n",
        "else:\n",
        "    selected_models.append(model1)\n",
        "\n",
        "if (recommended_model2 != \"None\"):\n",
        "    selected_models.append(recommended_model2)\n",
        "else:\n",
        "    if (model2 != \"None\"):\n",
        "        selected_models.append(model2)\n",
        "\n",
        "\n",
        "# I'm leaving this next line in case you want to set more than 2 models or whatever weird thing you want\n",
        "# simply uncomment it and it will ignore the selections above.\n",
        "# It is set the models I run when I set it to more than 1, but you can change it to whatever you want\n",
        "#selected_models = [\"ICBINP - I Can't Believe It's Not Photography\", \"Deliberate\", \"Anything Diffusion\", \"Dreamshaper\"]\n",
        "\n",
        "###\n",
        "#@markdown ---\n",
        "#@markdown ### Models not supported by the Horde\n",
        "\n",
        "outside_model = False #@param {type:\"boolean\"}\n",
        "outside_model_name = \"Van Gogh Diffusion\" #@param {type:\"string\"}\n",
        "model_file_name = \"Van\" #@param {type:\"string\"}\n",
        "baseline = \"SD 1.5\" # @param [\"SD 1.5\", \"SD XL\"]\n",
        "outside_model_download_url = \"https://huggingface.co/mirroring/horde_models/resolve/main/Van-Gogh-Diffusion.ckpt?download=true\" #@param {type:\"string\"}\n",
        "#@markdown ##### - To use unsupported/deprecated models (Segmind, Van Gogh Diffusion, etc)\n",
        "#@markdown ##### - For personal use, not to serve these models to the public\n",
        "#@markdown ##### - The worker is configured to activate Maintenance Mode when set to use outside models, this is to protect the owner\n",
        "#@markdown ##### - For this to work, all fields above must be filled:\n",
        "#@markdown ##### -1- outside_model, switch/toggle, a necessary check\n",
        "#@markdown ##### -2- outside_model_name, only for you to know what model is being served\n",
        "#@markdown ##### -3- model_file_name, the worker needs at least a part of the file's name to function\n",
        "#@markdown ##### -4- baseline, Stable Diffusion 1.5 or XL, necessary\n",
        "#@markdown ##### -4- outside_model_download_url, the download URL for the model you want to use. With civitai, check under \"1-2-3-x file\" and copy the download link there\n",
        "\n",
        "if(\"PickleTensor\" in outside_model_download_url or \".ckpt\" in outside_model_download_url):\n",
        "    file_extension = \".ckpt\"\n",
        "if(\"SafeTensor\" in outside_model_download_url or \".safetensors\" in outside_model_download_url):\n",
        "    file_extension = \".safetensors\"\n",
        "\n",
        "civitai = \"civitai.com/api/download/models\"\n",
        "if (civitai in outside_model_download_url):\n",
        "    outside_model_download_url = outside_model_download_url.split(\"?\")[0] + \"?token=\" + civitai_token\n",
        "\n",
        "new_mod15_ckpt = {\n",
        "    'model_name': \"Anything Diffusion\",\n",
        "    'file_name': \"Anything-Diffusion.ckpt\",\n",
        "    'sha256': \"633c153d96230355efb4230da6ae2e3ba85b084b93c89eb88cb1118d6cc06cef *Anything-Diffusion.sha256\",\n",
        "    'sha_name': \"Anything-Diffusion.sha256\"\n",
        "}\n",
        "new_mod15_safe = {\n",
        "    'model_name': \"Dreamshaper\",\n",
        "    'file_name': \"Dreamshaper.safetensors\",\n",
        "    'sha256': \"879db523c30d3b9017143d56705015e15a2cb5628762c11d086fed9538abd7fd *Dreamshaper.sha256\",\n",
        "    'sha_name': \"Dreamshaper.sha256\"\n",
        "}\n",
        "new_modxl_safe = {\n",
        "    'model_name': \"AlbedoBase XL (SDXL)\",\n",
        "    'file_name': \"albedo_base_xl.safetensors\",\n",
        "    'sha256': \"1718B5BB2DA1EF4815FEE8AF8A7FC2FA8AB8F467B279EDED4D991EA0CCE59A6D *albedo_base_xl.sha256\",\n",
        "    'sha_name': \"albedo_base_xl.sha256\"\n",
        "}\n",
        "if (baseline == \"SD XL\"):\n",
        "    new_mod = new_modxl_safe\n",
        "if (baseline == \"SD 1.5\"):\n",
        "    if(file_extension == \".ckpt\"):\n",
        "        new_mod = new_mod15_ckpt\n",
        "    if(file_extension == \".safetensors\"):\n",
        "        new_mod = new_mod15_safe\n",
        "\n",
        "if (outside_model and outside_model_name and outside_model_download_url):\n",
        "    selected_models = []\n",
        "    selected_models.append(new_mod['model_name'])\n",
        "    print (\"Running model not supported by the Horde (non-Customizer Role)\")\n",
        "    print (f'''Model: {outside_model_name:}, Horde Model: {new_mod['model_name']}, format: {new_mod['file_name'].split(\".\")[1]}''')\n",
        "# To actually enable models not supported by the Horde\n",
        "\n",
        "\n",
        "\n",
        "models_to_load = list(set(selected_models))\n",
        "print(f'Running: {models_to_load}')\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "allow_img2img = True #@param {type:\"boolean\"}\n",
        "allow_painting = True #@param {type:\"boolean\"}\n",
        "allow_lora = True #@param {type:\"boolean\"}\n",
        "allow_controlnet = False #@param {type:\"boolean\"}\n",
        "allow_post_processing = False #@param {type:\"boolean\"}\n",
        "#@markdown  ### I recommend you don't change these next 2 settings if you have no idea what you are doing\n",
        "queue_size = \"1\" # @param [0, 1, 2, 3, 4, 5]\n",
        "max_threads = \"1\" # @param [1, 2, 3]\n",
        "max_batch = \"5\" # @param [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
        "#@markdown ---\n",
        "\n",
        "safety_on_gpu = True\n",
        "nsfw = True\n",
        "censor_nsfw = False\n",
        "\n",
        "##############################\n",
        "#@markdown ## OPTIONAL, enabled by default\n",
        "#@markdown Disable default loras (reason: they are both obsolete and outdated)\n",
        "edit_lora_py_change_default_loras_bool = True #@param {type:\"boolean\"}\n",
        "#@markdown Increase Lora size limit to 1GB (reason: good donwload speed and storage capacity)\n",
        "edit_lora_py_increase_lora_size_limit_bool = True #@param {type:\"boolean\"}\n",
        "\n",
        "#markdown ## HIGHLY EXPERIMENTAL - OPTIONAL, disabled by default ####\n",
        "#markdown ##### Colab exclusive, for now, inject loras and textual inversions through prompt so they can be used in similar fashion to A1111\n",
        "#markdown ##### For loras: <lora:civitaiID or name:lora_strength:lora_clip:is_version, Anything you want here, to identify this lora>\n",
        "#markdown ##### Explaining is_version: 1--->True, 0--->False; If 0, use as always, put the civitAI ID and done; If 1, then you need the modelVersion (found in the URL on civitAI) instead of civitAI ID\n",
        "#markdown ##### Example with Add more details lora: \"<lora:82098:0.4:1.0:0, Add More Details>\": \"82098\", civitaiID; \"0.4\", lora model strength; \"1.0\", lora clip Strength; \"0\", is_version (0 means False);\n",
        "#markdown ##### For textual inversions: (embedding:civitaiID:ti_strength)\n",
        "#experimental = False #@param {type:\"boolean\"}\n",
        "#markdown ##### why? because some front-ends don't support loras/tis, so instead we let the worker handle that part and now EVERYONE can use them,\n",
        "#markdown ##### no matter what client they use to access the horde\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#markdown #### Obsolete while I set it up.\n",
        "beta_switch = False\n",
        "#markdown #### (RECOMMENDED) Enable this one to use the last stable version of the worker confirmed to work.\n",
        "#markdown #### IMPORTANT - If neither switch is set, it will download the current worker\n",
        "last_tested = False\n",
        "#markdown #### The maximum number of jobs the worker can do in a batch. I can confirm that up to 5 works, but there are very few batch requests for now so I don't yet know the best value for this.\n",
        "#markdown #### \"The horde will not give your max_batch at your max resolution, in order to avoid running out of VRAM. The Horde will assume you can fulfil your max batch at HALF your max resolution\"\n",
        "#markdown ---\n",
        "\n",
        "####For right now, THESE are the only variables that we care about\n",
        "\n",
        "\n",
        "horde_url = \"https://aihorde.net\"\n",
        "dynamic_models = False\n",
        "models_to_skip = [\"stable_diffusion_inpainting\", \"stable_diffusion_2.1\",  \"stable_diffusion_2.0\"]\n",
        "\n",
        "priority_usernames = []\n",
        "blacklist = []\n",
        "censorlist = []\n",
        "\n",
        "allow_unsafe_ip = True\n",
        "number_of_dynamic_models = 0\n",
        "max_models_to_download = 10\n",
        "forms = [\"caption\",\"nsfw\",\"interrogation\",\"post-process\"]\n",
        "\n",
        "\n",
        "current_path = \"/content/\"\n",
        "worker_path = current_path + \"horde-worker-reGen/\"\n",
        "bridgeData_file = worker_path + \"bridgeData.yaml\"\n",
        "notebook_version = \"23-05-2024\"\n",
        "\n",
        "#@markdown To reduce the level of verbosity/show shorter logs (Colab uses more and more of your PC RAM as the logs grow longer, but this will help with that)\n",
        "Short_Logs = False #@param {type:\"boolean\"}\n",
        "shorter_logs = Short_Logs\n",
        "#@markdown You can set here how often to clear logs (output only), in seconds. If left empty, logs will not be cleared\n",
        "interval = \"120\" #@param [\"\", 20, 30, 60, 120, 300, 1800, 3600]\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown To set maintenance mode ON/OFF on rerunning.\n",
        "Maintenance_Mode = False # @param {type:\"boolean\"}\n",
        "#@markdown Will be set to ON if running Outside Models. You need to keep your worker in Maintenance Mode with models not supported by the Horde, otherwise your worker will take people's request as normal and someone WILL report your worker for not giving the expected results.\n",
        "\n",
        "if not os.path.exists(bridgeData_file):\n",
        "    print (\"bridgeData.yaml file not found. Proceed to install the worker.\")\n",
        "else:\n",
        "    print (\"bridgeData.yaml file found. Recreating bridgeData.yaml and restarting the worker.\")\n",
        "    create_yaml()\n",
        "\n",
        "    download_models_aria2c()\n",
        "\n",
        "    if (Maintenance_Mode):\n",
        "        Maintenance_Mode = True\n",
        "    else:\n",
        "        if (outside_model and outside_model_name and outside_model_download_url):\n",
        "            Maintenance_Mode = True\n",
        "        else:\n",
        "            Maintenance_Mode = False\n",
        "    Set_Maintenance_Mode_function(Maintenance_Mode)\n",
        "\n",
        "    !cd /content\n",
        "    !source ../regen/bin/activate;python download_models.py\n",
        "    !cd /content\n",
        "\n",
        "    # Stop any reamining threads, maybe\n",
        "    stop_thread = True\n",
        "    # Start the thread\n",
        "    stop_thread = False\n",
        "    try:\n",
        "        threading.Thread(target=clear_output_periodically, args=(int(interval),)).start()\n",
        "        print(f\"Logs (output only) will be cleared every {int(interval)} seconds\")\n",
        "    except:\n",
        "        print(\"Logs will not be deleted\")\n",
        "\n",
        "    if (Short_Logs):\n",
        "        !source ../regen/bin/activate;python run_worker.py -vv\n",
        "    else:\n",
        "        !source ../regen/bin/activate;python run_worker.py\n",
        "\n",
        "# Stop any reamining threads, maybe\n",
        "stop_thread = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gui1uAYrry8"
      },
      "outputs": [],
      "source": [
        "# @title Get the Worker OUT of Maintenance Mode { display-mode: \"form\" }\n",
        "# This cell is to get the worker out of maintenance before starting, since some people don't know how or where to do it\n",
        "# Should be alright to do it here since most errors for Colab workers are not the owner's fault\n",
        "\n",
        "import subprocess\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "\n",
        "def Set_Maintenance_Mode_function(Maintenance_Mode):\n",
        "\n",
        "    i = False #Worker does not match/exist flag\n",
        "\n",
        "    command_find_user = f'''curl -X 'GET' \\\n",
        "              '{horde_url}/api/v2/find_user' \\\n",
        "              -H 'accept: application/json' \\\n",
        "              -H 'apikey: {api_key}' \\\n",
        "              -H 'Client-Agent: unknown:0:unknown'\n",
        "            '''\n",
        "\n",
        "    find_user_response = subprocess.check_output(command_find_user, shell=True).decode('utf-8')\n",
        "    data_api_key = json.loads(find_user_response)\n",
        "\n",
        "    if not (\"message\" in find_user_response):\n",
        "        for worker_id in data_api_key['worker_ids']:\n",
        "            command_find_worker = f'''curl -X 'GET' \\\n",
        "                      '{horde_url}/api/v2/workers/{worker_id}' \\\n",
        "                      -H 'accept: application/json' \\\n",
        "                      -H 'apikey: {api_key}' \\\n",
        "                      -H 'Client-Agent: unknown:0:unknown'\n",
        "                    '''\n",
        "            find_worker_response = subprocess.check_output(command_find_worker, shell=True).decode('utf-8')\n",
        "            data_worker = json.loads(find_worker_response)\n",
        "            if (worker_name == data_worker['name']):\n",
        "                print(f'''Worker name: {worker_name}, Worker ID: {worker_id}''')\n",
        "                i = True\n",
        "                if (Maintenance_Mode):\n",
        "                  set_maintenance_mode = \"true\"\n",
        "                else:\n",
        "                  set_maintenance_mode = \"false\"\n",
        "                command_maintenance_mode = f'''curl -X 'PUT' \\\n",
        "                  '{horde_url}/api/v2/workers/{worker_id}' \\\n",
        "                  -H 'accept: application/json' \\\n",
        "                  -H 'apikey: {api_key}' \\\n",
        "                  -H 'Client-Agent: unknown:0:unknown' \\\n",
        "                  -H 'Content-Type: application/json' \\\n",
        "                  -d'''+ r''' '{\n",
        "                  \"maintenance\":''' + f''' {set_maintenance_mode}\n",
        "                ''' + r'''}'\n",
        "                '''\n",
        "                maintenance_mode_response = subprocess.check_output(command_maintenance_mode, shell=True).decode('utf-8')\n",
        "                print(maintenance_mode_response)\n",
        "        if not i:\n",
        "          print(f'''No worker with the name: {worker_name}, can't change Maintenance Mode. A new worker will be created with that name.''')\n",
        "          worker_id = 0\n",
        "    else:\n",
        "        print(f'''The API key \"{api_key}\" does not exist''')\n",
        "\n",
        "Maintenance_Mode = False\n",
        "Set_Maintenance_Mode_function(Maintenance_Mode)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVecpjOM1xPu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 1.- Virtual Environment  { display-mode: \"form\" }\n",
        "\n",
        "!apt-get update\n",
        "!apt install python3.10-venv\n",
        "!python -m venv regen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzwQDVd_1xPu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 2.- Remove the worker if it exists, then, Clone the regen worker { display-mode: \"form\" }\n",
        "\n",
        "!cd /content;rm -r horde-worker-reGen\n",
        "\n",
        "if (beta_switch):\n",
        "    #!cd /content;git clone -b niter https://github.com/Haidra-Org/horde-worker-reGen.git > /dev/null 2>&1\n",
        "    ## Deleting the next part when beta_switch is ready, too lazy to figure it out\n",
        "    !cd /content;git clone https://github.com/Haidra-Org/horde-worker-reGen.git > /dev/null 2>&1\n",
        "    if (last_tested):\n",
        "        %cd /content/horde-worker-reGen\n",
        "        version = \"0776b3967d566ac3826a8c2083d0c315caaf26a1\"\n",
        "        !cd /content/horde-worker-reGen;git checkout version > /dev/null 2>&1\n",
        "        print (f'Worker Version (hash): {version}')\n",
        "        %cd /content\n",
        "else:\n",
        "    !cd /content;git clone https://github.com/Haidra-Org/horde-worker-reGen.git > /dev/null 2>&1\n",
        "    if (last_tested):\n",
        "        %cd /content/horde-worker-reGen\n",
        "        version = \"0776b3967d566ac3826a8c2083d0c315caaf26a1\"\n",
        "        !cd /content/horde-worker-reGen;git checkout version > /dev/null 2>&1\n",
        "        print (f'Worker Version (hash): {version}')\n",
        "        %cd /content\n",
        "\n",
        "# Necessary for edits, etc\n",
        "requirements_path = \"/content/horde-worker-reGen/requirements.txt\"\n",
        "horde_engine_version = \"fail\"\n",
        "# read the file\n",
        "with open(requirements_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "found_line_number = -1\n",
        "search_horde_engine_version = \"horde_engine\"\n",
        "for i, line in enumerate(lines):\n",
        "    if search_horde_engine_version in line:\n",
        "        horde_engine_version = line\n",
        "        break\n",
        "\n",
        "if (\"fail\" in horde_engine_version):\n",
        "    print (\"Could not find hordelib version in requirements.txt\")\n",
        "\n",
        "import re\n",
        "horde_engine_version_numeric_part = re.sub(r'^\\D*', '', horde_engine_version).rstrip()\n",
        "print (f'Horde Engine Line: {horde_engine_version}')\n",
        "horde_engine_version = horde_engine_version_numeric_part\n",
        "horde_engine_url = \"https://raw.githubusercontent.com/Haidra-Org/hordelib/v\" + horde_engine_version + \"/hordelib/\"\n",
        "horde_engine_path = \"/content/regen/lib/python3.10/site-packages/hordelib/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC4lce-51xPv",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 3.- Install requirements { display-mode: \"form\" }\n",
        "\n",
        "!source regen/bin/activate;pip install -r .\\/horde-worker-reGen/requirements.txt\n",
        "\n",
        "# Fix for cuda 11.8 no longer required\n",
        "#!source regen/bin/activate;pip install -r .\\/horde-worker-reGen/requirements.118.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-I1XNxx1xPv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 4.- Create .yaml config file { display-mode: \"form\" }\n",
        "\n",
        "import os\n",
        "\n",
        "%cd $worker_path\n",
        "print (\"Creating bridgeData.yaml file.\")\n",
        "\n",
        "def create_yaml():\n",
        "\n",
        "    from yaml import load, dump\n",
        "\n",
        "    def make_yaml_sublist(list_to_convert: list[str]):\n",
        "        sublist_yaml = dump(list_to_convert)\n",
        "        sublist_yaml = \"\\n\" + sublist_yaml\n",
        "        return sublist_yaml\n",
        "\n",
        "\n",
        "\n",
        "    data = f\"\"\"horde_url: \"{horde_url}\"\n",
        "api_key: \"{api_key}\"\n",
        "civitai_api_token: \"{civitai_token}\"\n",
        "priority_usernames: []\n",
        "max_threads: {max_threads}\n",
        "queue_size: {queue_size}\n",
        "max_batch: {max_batch}\n",
        "safety_on_gpu: {safety_on_gpu}\n",
        "require_upfront_kudos: false\n",
        "cycle_process_on_model_change: true\n",
        "dreamer_name: \"{worker_name}\"\n",
        "max_power: {max_power}\n",
        "nsfw: {nsfw.__str__().lower()}\n",
        "censor_nsfw: {censor_nsfw}\n",
        "blacklist: {blacklist}\n",
        "censorlist: {censorlist}\n",
        "allow_img2img: {allow_img2img.__str__().lower()}\n",
        "allow_painting: {allow_painting.__str__().lower()}\n",
        "allow_unsafe_ip: true\n",
        "allow_post_processing: {allow_post_processing.__str__().lower()}\n",
        "allow_controlnet: {allow_controlnet.__str__().lower()}\n",
        "allow_lora: {allow_lora.__str__().lower()}\n",
        "max_lora_cache_size: 20\n",
        "dynamic_models: false\n",
        "number_of_dynamic_models: 0\n",
        "max_models_to_download: 10\n",
        "stats_output_frequency: 30\n",
        "cache_home: \"./\"\n",
        "always_download: true\n",
        "temp_dir: \"./tmp\"\n",
        "disable_terminal_ui: True\n",
        "vram_to_leave_free: \"80%\"\n",
        "ram_to_leave_free: \"80%\"\n",
        "disable_disk_cache: false\n",
        "models_to_load: {make_yaml_sublist(models_to_load)}\n",
        "models_to_skip: {make_yaml_sublist(models_to_skip)}\n",
        "suppress_speed_warnings: false\n",
        "forms:\n",
        "- \"caption\"\n",
        "- \"nsfw\"\n",
        "- \"interrogation\"\n",
        "- \"post-process\"\n",
        "\"\"\"\n",
        "\n",
        "    with open(bridgeData_file, \"w\") as text_file:\n",
        "        text_file.write(data)\n",
        "\n",
        "    print (\"bridgeData.yaml file created.\")\n",
        "\n",
        "create_yaml()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tty0-ljnrry_"
      },
      "outputs": [],
      "source": [
        "# @title Download models using aria2c { display-mode: \"form\" }\n",
        "\n",
        "model_reference = \"/content/stable_diffusion.json\"\n",
        "\n",
        "# create compvis folder\n",
        "!mkdir -p /content/horde-worker-reGen/compvis\n",
        "\n",
        "# download the model reference if necessary\n",
        "import os\n",
        "if os.path.exists(model_reference):\n",
        "    print(\"Model Reference exists.\")\n",
        "else:\n",
        "    print(\"Downloading Model Reference.\")\n",
        "    !wget https://raw.githubusercontent.com/Haidra-Org/AI-Horde-image-model-reference/main/stable_diffusion.json -O /content/stable_diffusion.json\n",
        "\n",
        "# install aria2c if necessary\n",
        "import subprocess\n",
        "try:\n",
        "    # Run the command\n",
        "    output = subprocess.check_output([\"which\", \"aria2c\"])\n",
        "    # unnecessary to do this next step, but whatever, I can use the output from above with this\n",
        "    output = output.decode(\"utf-8\")\n",
        "    print(\"aria2c is installed.\")\n",
        "except:\n",
        "    print(f\"Installing aria2c.\")\n",
        "    !apt-get install -y aria2\n",
        "\n",
        "\n",
        "# function to look in the model reference for the selected models\n",
        "import json\n",
        "def find_matching_entries(filename, key, value_list):\n",
        "    \"\"\"\n",
        "    Finds all entries in a JSON file that match a given key and value list.\n",
        "\n",
        "    Args:\n",
        "      filename: The path to the JSON file.\n",
        "      key: The key to search for.\n",
        "      value_list: A list of values to match.\n",
        "\n",
        "    Returns:\n",
        "      A list of entries that match the criteria.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    matching_entries = []\n",
        "    for entry in data.items():\n",
        "        #print(entry)\n",
        "        if entry[0] in value_list:\n",
        "            matching_entries.append(entry)\n",
        "\n",
        "    return matching_entries\n",
        "\n",
        "\n",
        "# function to download the models with aria2c\n",
        "def download_models_aria2c():\n",
        "\n",
        "    filename = model_reference\n",
        "    key = \"name\"\n",
        "    value_list = selected_models\n",
        "    # call the find function\n",
        "    matching_entries = find_matching_entries(filename, key, value_list)\n",
        "\n",
        "    # download the model(s) and create the hash file(s)\n",
        "    %cd $worker_path\n",
        "    for entry in matching_entries:\n",
        "        #download link\n",
        "        if (\"civitai\" in entry[1][\"config\"]['download'][0]['file_url']):\n",
        "            entry[1][\"config\"]['download'][0]['file_url'] = entry[1][\"config\"]['download'][0]['file_url'].split(\"?\")[0] + \"?token=\" + civitai_token\n",
        "        if (outside_model and outside_model_name and outside_model_download_url and entry[0] in mask['model_name']):\n",
        "            print(f\"Outside model:{outside_model_name}\")\n",
        "            download_link = outside_model_download_url\n",
        "        else:\n",
        "            download_link = entry[1][\"config\"]['download'][0]['file_url']\n",
        "        print(download_link)\n",
        "        # file name\n",
        "        file_name = entry[1][\"config\"]['download'][0]['file_name']\n",
        "        print(file_name)\n",
        "        file_path = \"compvis/\" + file_name\n",
        "        # sha256\n",
        "        sha_name = file_name.split(\".\")[0] + \".sha256\"\n",
        "        sha = entry[1][\"config\"]['files'][0]['sha256sum']\n",
        "        sha = sha + \" *\" + sha_name\n",
        "        sha_path = \"/content/horde-worker-reGen/compvis/\" + sha_name\n",
        "        print(sha)\n",
        "        # donwnload model and create sha file\n",
        "        !aria2c -c -x 16 -s 16 -p $download_link -o $file_path\n",
        "        !rm $sha_path\n",
        "        !echo {sha} > $sha_path\n",
        "\n",
        "download_models_aria2c()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzw4Fik7rry_"
      },
      "outputs": [],
      "source": [
        "# @title ## Function: download file(s) { display-mode: \"form\" }\n",
        "\n",
        "# Simple download function with wget\n",
        "def download_file_wget(install_path, download_url):\n",
        "    !wget -O {install_path} {download_url}\n",
        "\n",
        "\n",
        "# define names, paths and urls for the files to download\n",
        "\n",
        "# horde engine files\n",
        "model_manager_folder = \"model_manager/\"\n",
        "name_lora_py = \"lora.py\"\n",
        "url_lora_py = horde_engine_url + model_manager_folder + name_lora_py\n",
        "path_lora_py = horde_engine_path + model_manager_folder + name_lora_py\n",
        "\n",
        "def download_files():\n",
        "    if (download_lora_py_bool):\n",
        "        download_file_wget(install_path = path_lora_py, download_url = url_lora_py)\n",
        "\n",
        "# Use the switches/toggles to download the file(s) as necessary\n",
        "#@markdown Only to download again the files affected by the edits, effectively reverting any changes made to them. Only for emergencies or if you know what you are doing\n",
        "download_lora_py_bool = False #@param {type:\"boolean\"}\n",
        "\n",
        "download_files()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKfqDPx1rrzA"
      },
      "outputs": [],
      "source": [
        "# @title ## Function: edit file(s) { display-mode: \"form\" }\n",
        "def Edit_file_code(goal, target_file, target_code, new_code, lines_offset, pop_x_lines):\n",
        "\n",
        "    print(f'''-------------------------------------\n",
        "Purpose: {goal}\n",
        "target file: {target_file}\n",
        "target code:\\n{target_code}\n",
        "new code:\\n{new_code}\n",
        "lines offset: {lines_offset}\n",
        "{pop_x_lines} lines to pop''')\n",
        "\n",
        "    # check if the edit was already done, exit if so\n",
        "    with open(target_file, 'r') as file:\n",
        "        data = file.read()\n",
        "    if (new_code in data):\n",
        "        print (f'''The new code was found in target file: {target_file}. The edit was already done. Exiting.''')\n",
        "        return\n",
        "\n",
        "    # read the file\n",
        "    with open(target_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    found_line_number = -1\n",
        "\n",
        "    # find the target line \"target_code\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if target_code in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code\" to the data\n",
        "    if found_line_number != -1:\n",
        "        if (pop_x_lines):\n",
        "            print (\"haa\")\n",
        "            for i in range(pop_x_lines):\n",
        "                lines.pop(found_line_number)\n",
        "        lines.insert(found_line_number + lines_offset, new_code)\n",
        "        print (f'''New code added to target file: {target_file}. File edited.''')\n",
        "    else:\n",
        "        print (f'''Could not find target code: {target_code}.''')\n",
        "\n",
        "    # rewrite target_file\n",
        "    with open(target_file, 'w') as file:\n",
        "        file.writelines(lines)\n",
        "\n",
        "\n",
        "# Changes/edits to the worker\n",
        "\"\"\"\n",
        "    Change default loras, ignore the long, useless, outdated, deprecated list\n",
        "\"\"\"\n",
        "def edit_lora_py_change_default_loras():\n",
        "    # variables\n",
        "    goal = \"Change default Loras list for mine\"\n",
        "    target_file = path_lora_py\n",
        "    target_code = '''            self._default_lora_ids = self._get_json(self.LORA_DEFAULTS)'''\n",
        "    new_code = r'''            self._default_lora_ids = [216620, 216590]\n",
        "'''\n",
        "    lines_offset = 0\n",
        "    pop_x_lines = 1\n",
        "    # call the edit file function\n",
        "    Edit_file_code(goal, target_file, target_code, new_code, lines_offset, pop_x_lines)\n",
        "\n",
        "\"\"\"\n",
        "    Increase Lora size limit to 1GB\n",
        "\"\"\"\n",
        "def edit_lora_py_increase_lora_size_limit():\n",
        "    # variables\n",
        "    goal = \"Increase Lora size limit to 1GB\"\n",
        "    target_file = path_lora_py\n",
        "    target_code = '''            and lora[\"versions\"][lora_version][\"size_mb\"] > 220'''\n",
        "    new_code = r'''            and lora[\"versions\"][lora_version][\"size_mb\"] > 1024\n",
        "'''\n",
        "    lines_offset = 0\n",
        "    pop_x_lines = 1\n",
        "    # call the edit file function\n",
        "    Edit_file_code(goal, target_file, target_code, new_code, lines_offset, pop_x_lines)\n",
        "\n",
        "\n",
        "# Execute all enabled edits\n",
        "def edit_files():\n",
        "    if (edit_lora_py_change_default_loras_bool):\n",
        "        edit_lora_py_change_default_loras()\n",
        "    if (edit_lora_py_increase_lora_size_limit_bool):\n",
        "        edit_lora_py_increase_lora_size_limit()\n",
        "\n",
        "edit_files()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwMyg3JN1xPv",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 6.- Run download models { display-mode: \"form\" }\n",
        "\n",
        "# Make sure you have the correct path based on any `cd` commands above\n",
        "!cd /content\n",
        "\n",
        "!source ../regen/bin/activate;python download_models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRnzqILb1xPw",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 7.- Run the worker { display-mode: \"form\" }\n",
        "\n",
        "# Make sure you have the correct path based on any `cd` commands above\n",
        "!cd /content\n",
        "\n",
        "if (outside_model and outside_model_name and outside_model_download_url):\n",
        "    Maintenance_Mode = True\n",
        "Set_Maintenance_Mode_function(Maintenance_Mode)\n",
        "\n",
        "# Stop any reamining threads, maybe\n",
        "stop_thread = True\n",
        "# Start the thread\n",
        "stop_thread = False\n",
        "# To periodically clear the logs\n",
        "# @markdown If this interval is changed to anything other than \"ignore\", it will override the value set on the first cell\n",
        "interval2 = \"ignore\" #@param [\"ignore\", \"\", 20, 30, 60, 120, 300, 1800, 3600]\n",
        "if (\"ignore\" in interval2):\n",
        "    try:\n",
        "        threading.Thread(target=clear_output_periodically, args=(int(interval),)).start()\n",
        "        print(f\"Logs (output only) will be cleared every {int(interval)} seconds\")\n",
        "    except:\n",
        "        print(\"Logs will not be deleted\")\n",
        "else:\n",
        "    try:\n",
        "        threading.Thread(target=clear_output_periodically, args=(int(interval2),)).start()\n",
        "        print(f\"Logs (output only) will be cleared every {int(interval2)} seconds\")\n",
        "    except:\n",
        "        print(\"Logs will not be deleted\")\n",
        "\n",
        "\n",
        "if (shorter_logs):\n",
        "    !source ../regen/bin/activate;python run_worker.py -vv\n",
        "else:\n",
        "    !source ../regen/bin/activate;python run_worker.py\n",
        "\n",
        "# Stop the thread when the cell is stopped.\n",
        "stop_thread = True"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}